{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "This notebook shows the process of using the Chronicling America API to search for a selection of newspapers and download the XML OCR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # for retrieving web data\n",
    "from xml.etree import ElementTree as ET # for parsing XML\n",
    "import os # for file handling\n",
    "\n",
    "import time # for respecting rate limits\n",
    "\n",
    "import time\n",
    "from threading import Lock\n",
    "from concurrent import futures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals and Config\n",
    "This cell defines global variables and notebook-wide configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True # set this flag for detailed output from cells.\n",
    "\n",
    "DATA_DIR = './data/'   # path to data directory\n",
    "TXT_PATH = 'pages.txt' # path to text file to hold page IDs\n",
    "XML_DIR = 'xml/'       # path to directory within data directory to store XML files\n",
    "\n",
    "DATA_URL   = 'https://chroniclingamerica.loc.gov/' # path for file downloads\n",
    "SEARCH_URL = 'https://chroniclingamerica.loc.gov/search/pages/results/' # path for id search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Pages\n",
    "This cell uses the [Chronicling America API](https://chroniclingamerica.loc.gov/about/api/) to find relevant pages from the collection. The example below retrieves a JSON summary of the first 1000 results for New York newspapers containing the word \"California\" between 1900 and 1914 and stores the ID of each page in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received response for page 1, containing items 1-21\n",
      "received response for page 2, containing items 21-41\n",
      "received response for page 3, containing items 41-61\n",
      "received response for page 4, containing items 61-81\n",
      "received response for page 5, containing items 81-101\n",
      "received response for page 6, containing items 101-121\n",
      "received response for page 7, containing items 121-141\n",
      "received response for page 8, containing items 141-161\n",
      "received response for page 9, containing items 161-181\n",
      "received response for page 10, containing items 181-201\n",
      "received response for page 11, containing items 201-221\n",
      "received response for page 12, containing items 221-241\n",
      "received response for page 13, containing items 241-261\n",
      "received response for page 14, containing items 261-281\n",
      "received response for page 15, containing items 281-301\n",
      "received response for page 16, containing items 301-321\n",
      "received response for page 17, containing items 321-341\n",
      "received response for page 18, containing items 341-361\n",
      "received response for page 19, containing items 361-381\n",
      "received response for page 20, containing items 381-401\n",
      "received response for page 21, containing items 401-421\n",
      "received response for page 22, containing items 421-441\n",
      "received response for page 23, containing items 441-461\n",
      "received response for page 24, containing items 461-481\n",
      "received response for page 25, containing items 481-501\n",
      "received response for page 26, containing items 501-521\n",
      "received response for page 27, containing items 521-541\n",
      "received response for page 28, containing items 541-561\n",
      "received response for page 29, containing items 561-581\n",
      "received response for page 30, containing items 581-601\n",
      "received response for page 31, containing items 601-621\n",
      "received response for page 32, containing items 621-641\n",
      "received response for page 33, containing items 641-661\n",
      "received response for page 34, containing items 661-681\n",
      "received response for page 35, containing items 681-701\n",
      "received response for page 36, containing items 701-721\n",
      "received response for page 37, containing items 721-741\n",
      "received response for page 38, containing items 741-761\n",
      "received response for page 39, containing items 761-781\n",
      "received response for page 40, containing items 781-801\n",
      "received response for page 41, containing items 801-821\n",
      "received response for page 42, containing items 821-841\n",
      "received response for page 43, containing items 841-861\n",
      "received response for page 44, containing items 861-881\n",
      "received response for page 45, containing items 881-901\n",
      "received response for page 46, containing items 901-921\n",
      "received response for page 47, containing items 921-941\n",
      "received response for page 48, containing items 941-961\n",
      "received response for page 49, containing items 961-981\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mJSONDecodeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17125/4266450114.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTXT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_pages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{SEARCH_URL}?{base_qstr}&page={n}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'received response for page {n}, containing items {response[\"startIndex\"]}-{response[\"startIndex\"] + 20}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;31m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m             \u001b[0;31m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRequestsJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    \"state\": \"New York\",\n",
    "\n",
    "    \"dateFilterType\" : \"yearRange\",\n",
    "    \"date1\"          : \"1900\",\n",
    "    \"date2\"          : \"1914\",\n",
    "\n",
    "    \"ortext\"     : \"\",\n",
    "    \"andtext\"    : \"California\",\n",
    "    \"phrasetext\" : \"\"\n",
    "}\n",
    "n_pages = 50 # 1000 items at 20 items per response\n",
    "\n",
    "base_qstr = '&'.join(f'{key}={value.replace(\" \", \"+\")}' for key, value in options.items()) + '&format=json'\n",
    "\n",
    "with open(DATA_DIR + TXT_PATH, 'w') as fp:\n",
    "    for n in range(1, n_pages + 1):\n",
    "        response = requests.get(f'{SEARCH_URL}?{base_qstr}&page={n}').json()\n",
    "        if verbose:\n",
    "            print(f'received response for page {n}, containing items {response[\"startIndex\"]}-{response[\"startIndex\"] + 20}')\n",
    "        for item in response['items']:\n",
    "            fp.write(item[\"id\"] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting XML\n",
    "This cell retrieves OCR XML files from list of page IDs generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading xml for 980 page IDs\n",
      "Rate limit reached; waiting 80.19 seconds\n",
      "Rate limit reached; waiting 79.92 seconds\n",
      "Rate limit reached; waiting 79.73 seconds\n",
      "Rate limit reached; waiting 79.57 seconds\n",
      "Rate limit reached; waiting 1.92 seconds\n",
      "Rate limit reached; waiting 1.85 seconds\n",
      "Rate limit reached; waiting 0.47 seconds\n",
      "Rate limit reached; waiting 0.23 seconds\n",
      "Rate limit reached; waiting 76.55 seconds\n",
      "Rate limit reached; waiting 76.52 seconds\n",
      "Rate limit reached; waiting 76.02 seconds\n",
      "Rate limit reached; waiting 1.76 seconds\n",
      "Rate limit reached; waiting 0.60 seconds\n",
      "Rate limit reached; waiting 0.44 seconds\n",
      "bad response for https://chroniclingamerica.loc.gov//lccn/sn83030214/1910-12-25/ed-1/seq-30/ocr.xml: HTTP code 520\n",
      "Rate limit reached; waiting 68.43 seconds\n",
      "Rate limit reached; waiting 67.62 seconds\n",
      "Rate limit reached; waiting 67.15 seconds\n",
      "Rate limit reached; waiting 67.11 seconds\n",
      "Rate limit reached; waiting 1.51 seconds\n",
      "Rate limit reached; waiting 1.26 seconds\n",
      "Rate limit reached; waiting 1.00 seconds\n",
      "Rate limit reached; waiting 0.66 seconds\n",
      "Rate limit reached; waiting 6.72 seconds\n",
      "Rate limit reached; waiting 6.69 seconds\n",
      "Rate limit reached; waiting 6.68 seconds\n",
      "Rate limit reached; waiting 6.67 seconds\n",
      "Rate limit reached; waiting 0.31 seconds\n",
      "Rate limit reached; waiting 0.24 seconds\n",
      "Rate limit reached; waiting 1.11 seconds\n",
      "Rate limit reached; waiting 1.11 seconds\n",
      "Rate limit reached; waiting 0.78 seconds\n",
      "Rate limit reached; waiting 0.68 seconds\n",
      "Rate limit reached; waiting 0.01 seconds\n",
      "Rate limit reached; waiting 1.12 seconds\n",
      "Rate limit reached; waiting 0.23 seconds\n",
      "Rate limit reached; waiting 0.21 seconds\n",
      "Rate limit reached; waiting 0.14 seconds\n",
      "Rate limit reached; waiting 70.19 seconds\n",
      "Rate limit reached; waiting 69.49 seconds\n",
      "Rate limit reached; waiting 69.19 seconds\n",
      "Rate limit reached; waiting 69.17 seconds\n",
      "Processed 980 ids in 363.67 seconds: 89 downloaded, 890 already present, 1 failed\n"
     ]
    }
   ],
   "source": [
    "SUCCESS, SKIP, FAIL = 0, 1, 2\n",
    "\n",
    "BURST_WINDOW, BURST_MAX = 90, 20\n",
    "CRAWL_WINDOW, CRAWL_MAX = 15, 20\n",
    "\n",
    "def download_many_xml(ids: list[str], max_workers=4) -> int:\n",
    "    burst_times: list[float] = []\n",
    "    crawl_times: list[float] = []\n",
    "    burst_lock = Lock()\n",
    "    crawl_lock = Lock()\n",
    "\n",
    "    def record():\n",
    "        \"\"\"Record timestamp when request is made.\"\"\"\n",
    "        nonlocal burst_times, crawl_times\n",
    "        with burst_lock: burst_times.append(time.time())\n",
    "        with crawl_lock: crawl_times.append(time.time())\n",
    "    \n",
    "    def check_limits() -> tuple[float, float]:\n",
    "        \"\"\"Check rate limits and return required wait time if any.\"\"\"\n",
    "        nonlocal burst_times, crawl_times\n",
    "\n",
    "        # remove old timestamps\n",
    "        now = time.time()\n",
    "        burst_times = [t for t in burst_times if now - t < BURST_WINDOW]\n",
    "        crawl_times = [t for t in crawl_times if now - t < BURST_WINDOW]\n",
    "\n",
    "        # check burst limit\n",
    "        burst_wait = float(0)\n",
    "        with burst_lock:\n",
    "            now = time.time() # need to refresh since it might take time to get the lock\n",
    "            if len(burst_times) > BURST_MAX:\n",
    "                burst_wait = max(0, burst_times[0] + BURST_WINDOW - now)\n",
    "        \n",
    "        crawl_wait = float(0)\n",
    "        with crawl_lock:\n",
    "            now = time.time() # need to refresh since it might take time to get the lock\n",
    "            if len(burst_times) > BURST_MAX:\n",
    "                burst_wait = max(0, burst_times[0] + BURST_WINDOW - now)\n",
    "\n",
    "        return (burst_wait, crawl_wait)\n",
    "    \n",
    "    start = time.time()\n",
    "    if verbose:\n",
    "        print(f'Downloading xml for {len(ids)} page IDs')\n",
    "\n",
    "    def download_single(id: str):\n",
    "        if os.path.exists(f'{DATA_DIR}{XML_DIR}{id.replace(\"/\", \"\")}.xml'):\n",
    "            return SKIP\n",
    "    \n",
    "        wait_time = max(check_limits())\n",
    "        if wait_time > 0:\n",
    "            if verbose:\n",
    "                print(f'Rate limit reached; waiting {wait_time:.2f} seconds')\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        try:\n",
    "            record()\n",
    "            response = requests.get(f'{DATA_URL}{id}ocr.xml')\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError:\n",
    "            if response.status_code == 429:\n",
    "                print(f'ERROR: Rate limit exceeded for {DATA_URL}{id}ocr.xml')\n",
    "            else:\n",
    "                print(f'bad response for {DATA_URL}{id}ocr.xml: HTTP code', response.status_code)\n",
    "            return FAIL\n",
    "        \n",
    "        xml = ET.ElementTree(ET.fromstring(response.content))\n",
    "        ET.indent(xml, space=\"\\t\", level=0)\n",
    "\n",
    "        filename = id.replace('/', '')\n",
    "        with open(f'{DATA_DIR}{XML_DIR}{filename}.xml', 'w'): pass\n",
    "        xml.write(f'{DATA_DIR}{XML_DIR}{filename}.xml')\n",
    "\n",
    "        return SUCCESS\n",
    "    \n",
    "    with futures.ThreadPoolExecutor(max_workers) as exec:\n",
    "        outcomes = list(exec.map(download_single, ids))\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f'Processed {len(ids)} ids in {time.time() - start:.2f} seconds:',\n",
    "            f'{outcomes.count(SUCCESS)} downloaded,', \n",
    "            f'{outcomes.count(SKIP)} already present,', \n",
    "            f'{outcomes.count(FAIL)} failed'\n",
    "        )\n",
    "    return outcomes.count(SUCCESS) + outcomes.count(FAIL)\n",
    "\n",
    "id_list: list[str] = []\n",
    "with open(f'{DATA_DIR}{TXT_PATH}', 'r') as fp:\n",
    "    while(id := fp.readline()):\n",
    "        id_list.append(id[:-1])\n",
    "        \n",
    "n_success = download_many_xml(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://chroniclingamerica.loc.gov//lccn/sn83030193/1914-11-19/ed-1/seq-4/ocr.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Fri, 14 Feb 2025 02:41:13 GMT\n",
      "Content-Type: text/xml\n",
      "Transfer-Encoding: chunked\n",
      "Connection: keep-alive\n",
      "access-control-allow-origin: *\n",
      "referrer-policy: no-referrer-when-downgrade\n",
      "strict-transport-security: max-age=3600; preload\n",
      "x-content-type-options: nosniff\n",
      "last-modified: Fri, 20 May 2011 23:41:17 GMT\n",
      "Cache-Control: max-age=31536000\n",
      "expires: Sat, 14 Feb 2026 02:00:30 GMT\n",
      "content-disposition: inline; filename=service-ndnp-nn-batch_nn_janet_ver01-data-sn83030193-00280765983-1914111901-0460.xml\n",
      "x-app-cache-tag: storage-services, storage-services/service\n",
      "CF-Cache-Status: HIT\n",
      "Age: 2443\n",
      "Vary: Accept-Encoding\n",
      "Server: cloudflare\n",
      "CF-RAY: 9119b2a8dfa27bf5-LAX\n",
      "Content-Encoding: gzip\n"
     ]
    }
   ],
   "source": [
    "for key, value in response.headers.items():\n",
    "    print(key, value, sep=\": \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
