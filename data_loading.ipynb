{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the base URL for *Chronicling* and downloads the list of 2871 batch names as strings, with examples shown as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total batches: 2871\n",
      "examples:\n",
      "ak_albatross_ver01/\n",
      "ak_arcticfox_ver02/\n",
      "ak_arctictern_ver01/\n",
      "ak_belugawhale_ver01/\n",
      "ak_bluewhale_ver01/\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "BASE_URL = \"https://chroniclingamerica.loc.gov/data/batches/\"\n",
    "\n",
    "soup = BeautifulSoup(requests.get(BASE_URL).content, 'html.parser')\n",
    "batches_all = [batch.find('a').get_text() for batch in soup.find_all('tr')[2:]]\n",
    "print(f'total batches: {len(batches_all)}\\nexamples:')\n",
    "print(*batches_all[:5], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell samples 7 random batches from the full list of batches for exploratory analysis (seeded for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected batches:\n",
      "    ak_arcticfox_ver02/\n",
      "    ohi_lima_ver01/\n",
      "    me_bangor_ver02/\n",
      "    vtu_eden_ver01/\n",
      "    tu_carla_ver01/\n",
      "    dlc_bravo_ver01/\n",
      "    in_fairmount_ver02/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2002)\n",
    "batches =random.sample(batches_all, 7)\n",
    "print('selected batches:\\n   ', '\\n    '.join(batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell creates directories and summary files for selected batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote info for 8829 issues\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "issues = 0\n",
    "\n",
    "for batch in batches:\n",
    "    batch_xml = ET.fromstring(requests.get(f'{BASE_URL}{batch}data/batch.xml').content)\n",
    "\n",
    "    dir = f'./data/{batch}/'\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "\n",
    "    with open(f'{dir}batch_info.csv', 'w') as fp:\n",
    "        fp.write('lccn, issue_date, edition_order, path, url\\n')\n",
    "        for issue in batch_xml.findall('{http://www.loc.gov/ndnp}issue'):\n",
    "            lccn, issueDate, editionOrder = issue.attrib[\"lccn\"], issue.attrib[\"issueDate\"], issue.attrib[\"editionOrder\"]\n",
    "            url = '/'.join((issue.text or \"\").replace('./', '').split('/')[0:3]) + '/'\n",
    "            path = f'{lccn}{issueDate}{editionOrder}'\n",
    "            fp.write(f'{lccn},{issueDate},{editionOrder},{path},{url}\\n')\n",
    "\n",
    "            issues += 1\n",
    "        fp.write('{\\n')\n",
    "\n",
    "print(f'wrote info for {issues} issues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell estimates the storage requirements of the selected batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jp2 sizes (GB): [41.76291999999999, 8.65162, 36.166806666666666, 42.32690000000001, 56.06825, 31.964480000000002, 89.20483]\n",
      "pdf sizes (GB): [9.88739228, 1.2857075233333333, 12.174497440000003, 6.997127316666667, 8.8028941, 12.832773333333334, 9.504823900000002]\n",
      "total pages   : [11184.0, 1832.0, 9846.666666666668, 8922.666666666666, 12250.0, 4661.333333333333, 6996.0]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def get_gb(size: str):\n",
    "    number, unit = size.split(' ')\n",
    "    if unit == \"KB\":\n",
    "        return float(number) / 1e6\n",
    "    elif unit==\"MB\":\n",
    "        return float(number) / 1e3\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "jp2_sizes, pdf_sizes, total_pages = [], [], []\n",
    "for i, batch in enumerate(batches):\n",
    "    with open(f'./data/{batch}/batch_info.csv', 'r') as fp:\n",
    "        issue_jp2_sizes, issue_pdf_sizes, issue_pages = [], [], []\n",
    "\n",
    "        # sample 5 issues from batch\n",
    "        issues = list(csv.reader(fp))\n",
    "        sample_rows = random.sample(issues, 5)\n",
    "        for row in sample_rows:\n",
    "            sample_jp2_size, sample_pdf_size, sample_pages = 0, 0, 0\n",
    "\n",
    "            soup = BeautifulSoup(requests.get(f'{BASE_URL}{batch}data/{row[4]}').content, 'html.parser')\n",
    "            for row in soup.find_all('tr')[2:]:\n",
    "                format, size = row.find_all('td')[0].find('a').text.split('.')[1], row.find_all('td')[2].text\n",
    "                if format == 'jp2':\n",
    "                    sample_jp2_size += get_gb(size)\n",
    "                if format == 'pdf':\n",
    "                    sample_pdf_size += get_gb(size)\n",
    "                    sample_pages += 1\n",
    "            \n",
    "            issue_jp2_sizes.append(sample_jp2_size)\n",
    "            issue_pdf_sizes.append(sample_pdf_size)\n",
    "            issue_pages.append(sample_pages)\n",
    "        \n",
    "        # estimate total batch size, discarding lowest and highest samples\n",
    "        avg_jp2_size = (sum(issue_jp2_sizes) - min(issue_jp2_sizes) - max(issue_jp2_sizes)) / 3\n",
    "        avg_pdf_size = (sum(issue_pdf_sizes) - min(issue_pdf_sizes) - max(issue_pdf_sizes)) / 3\n",
    "        avg_pages    = (sum(issue_pages) - min(issue_pages) - max(issue_pages)) / 3\n",
    "\n",
    "        jp2_sizes.append(avg_jp2_size * len(issues))\n",
    "        pdf_sizes.append(avg_pdf_size * len(issues))\n",
    "        total_pages.append(avg_pages * len(issues))\n",
    "\n",
    "print(f'jp2 sizes (GB):', jp2_sizes)\n",
    "print(f'pdf sizes (GB):', pdf_sizes)\n",
    "print(f'total pages   :', total_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
