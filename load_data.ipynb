{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "This notebook shows the process of using the Chronicling America API to search for a selection of newspapers and download the XML OCR data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests # for retrieving web data\n",
    "from xml.etree import ElementTree as ET # for parsing XML\n",
    "import os # for file handling\n",
    "\n",
    "import time # for respecting rate limits\n",
    "\n",
    "import time\n",
    "from threading import Lock\n",
    "from concurrent import futures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals and Config\n",
    "This cell defines global variables and notebook-wide configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True # set this flag for detailed output from cells.\n",
    "\n",
    "DATA_DIR = './data/'   # path to data directory\n",
    "TXT_PATH = 'pages.txt' # path to text file to hold page IDs\n",
    "XML_DIR = 'xml/'       # path to directory within data directory to store XML files\n",
    "\n",
    "DATA_URL   = 'https://chroniclingamerica.loc.gov/' # path for file downloads\n",
    "SEARCH_URL = 'https://chroniclingamerica.loc.gov/search/pages/results/' # path for id search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Pages\n",
    "This cell uses the [Chronicling America API](https://chroniclingamerica.loc.gov/about/api/) to find relevant pages from the collection. The example below retrieves a JSON summary of the first 1000 results for New York newspapers containing the word \"California\" between 1900 and 1914 and stores the ID of each page in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "received response for page 1, containing items 1-21\n",
      "received response for page 2, containing items 21-41\n",
      "received response for page 3, containing items 41-61\n",
      "received response for page 4, containing items 61-81\n",
      "received response for page 5, containing items 81-101\n",
      "received response for page 6, containing items 101-121\n",
      "received response for page 7, containing items 121-141\n",
      "received response for page 8, containing items 141-161\n",
      "received response for page 9, containing items 161-181\n",
      "received response for page 10, containing items 181-201\n",
      "received response for page 11, containing items 201-221\n",
      "received response for page 12, containing items 221-241\n",
      "received response for page 13, containing items 241-261\n",
      "received response for page 14, containing items 261-281\n",
      "received response for page 15, containing items 281-301\n",
      "received response for page 16, containing items 301-321\n",
      "received response for page 17, containing items 321-341\n",
      "received response for page 18, containing items 341-361\n",
      "received response for page 19, containing items 361-381\n",
      "received response for page 20, containing items 381-401\n",
      "received response for page 21, containing items 401-421\n",
      "received response for page 22, containing items 421-441\n",
      "received response for page 23, containing items 441-461\n",
      "received response for page 24, containing items 461-481\n",
      "received response for page 25, containing items 481-501\n",
      "received response for page 26, containing items 501-521\n",
      "received response for page 27, containing items 521-541\n",
      "received response for page 28, containing items 541-561\n",
      "received response for page 29, containing items 561-581\n",
      "received response for page 30, containing items 581-601\n",
      "received response for page 31, containing items 601-621\n",
      "received response for page 32, containing items 621-641\n",
      "received response for page 33, containing items 641-661\n",
      "received response for page 34, containing items 661-681\n",
      "received response for page 35, containing items 681-701\n",
      "received response for page 36, containing items 701-721\n",
      "received response for page 37, containing items 721-741\n",
      "received response for page 38, containing items 741-761\n",
      "received response for page 39, containing items 761-781\n",
      "received response for page 40, containing items 781-801\n",
      "received response for page 41, containing items 801-821\n",
      "received response for page 42, containing items 821-841\n",
      "received response for page 43, containing items 841-861\n",
      "received response for page 44, containing items 861-881\n",
      "received response for page 45, containing items 881-901\n",
      "received response for page 46, containing items 901-921\n",
      "received response for page 47, containing items 921-941\n",
      "received response for page 48, containing items 941-961\n",
      "received response for page 49, containing items 961-981\n",
      "received response for page 50, containing items 981-1001\n"
     ]
    }
   ],
   "source": [
    "options = {\n",
    "    \"state\": \"New York\",\n",
    "\n",
    "    \"dateFilterType\" : \"yearRange\",\n",
    "    \"date1\"          : \"1900\",\n",
    "    \"date2\"          : \"1914\",\n",
    "\n",
    "    \"ortext\"     : \"\",\n",
    "    \"andtext\"    : \"California\",\n",
    "    \"phrasetext\" : \"\"\n",
    "}\n",
    "n_pages = 50 # 1000 items at 20 items per response\n",
    "\n",
    "base_qstr = '&'.join(f'{key}={value.replace(\" \", \"+\")}' for key, value in options.items()) + '&format=json'\n",
    "\n",
    "with open(DATA_DIR + TXT_PATH, 'w') as fp:\n",
    "    for n in range(1, n_pages + 1):\n",
    "        response = requests.get(f'{SEARCH_URL}?{base_qstr}&page={n}').json()\n",
    "        if verbose:\n",
    "            print(f'received response for page {n}, containing items {response[\"startIndex\"]}-{response[\"startIndex\"] + 20}')\n",
    "        for item in response['items']:\n",
    "            fp.write(item['id'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting XML\n",
    "This cell retrieves OCR XML files from list of page IDs generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUCCESS, SKIP, FAIL = 0, 1, 2\n",
    "\n",
    "BURST_WINDOW, BURST_MAX = 60, 20\n",
    "CRAWL_WINDOW, CRAWL_MAX = 10, 20\n",
    "\n",
    "def download_many_xml(ids: list[str], max_workers=4) -> int:\n",
    "    burst_times: list[float] = []\n",
    "    crawl_times: list[float] = []\n",
    "    burst_lock = Lock()\n",
    "    crawl_lock = Lock()\n",
    "\n",
    "    def record():\n",
    "        \"\"\"Record timestamp when request is made.\"\"\"\n",
    "        nonlocal burst_times, crawl_times\n",
    "        with burst_lock: burst_times.append(time.time())\n",
    "        with crawl_lock: crawl_times.append(time.time())\n",
    "    \n",
    "    def check_limits() -> tuple[float, float]:\n",
    "        \"\"\"Check rate limits and return required wait time if any.\"\"\"\n",
    "        nonlocal burst_times, crawl_times\n",
    "\n",
    "        # remove old timestamps\n",
    "        now = time.time()\n",
    "        burst_times = [t for t in burst_times if now - t < BURST_WINDOW]\n",
    "        crawl_times = [t for t in crawl_times if now - t < BURST_WINDOW]\n",
    "\n",
    "        # check burst limit\n",
    "        burst_wait = float(0)\n",
    "        with burst_lock:\n",
    "            now = time.time() # need to refresh since it might take time to get the lock\n",
    "            if len(burst_times) > BURST_MAX:\n",
    "                burst_wait = max(0, burst_times[0] + BURST_WINDOW - now)\n",
    "        \n",
    "        crawl_wait = float(0)\n",
    "        with crawl_lock:\n",
    "            now = time.time() # need to refresh since it might take time to get the lock\n",
    "            if len(burst_times) > BURST_MAX:\n",
    "                burst_wait = max(0, burst_times[0] + BURST_WINDOW - now)\n",
    "\n",
    "        return (burst_wait, crawl_wait)\n",
    "    \n",
    "    start = time.time()\n",
    "    if verbose:\n",
    "        print(f'Downloading xml for {len(ids)} page IDs')\n",
    "\n",
    "    def download_single(id: str):\n",
    "        if os.path.exists(f'{DATA_DIR}{XML_DIR}{id.replace(\"/\", \"\")}.xml'):\n",
    "            return SKIP\n",
    "    \n",
    "        wait_time = max(check_limits())\n",
    "        if wait_time > 0:\n",
    "            if verbose:\n",
    "                print(f'Rate limit reached; waiting {wait_time:.2f} seconds')\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "        try:\n",
    "            record()\n",
    "            response = requests.get(f'{DATA_URL}{id}ocr.xml')\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.HTTPError:\n",
    "            if response.status_code == 429:\n",
    "                print(f'ERROR: Rate limit exceeded')\n",
    "            else:\n",
    "                print(f'bad response for {DATA_URL}{id}ocr.xml: HTTP code', response.status_code)\n",
    "            return FAIL\n",
    "        \n",
    "        xml = ET.ElementTree(ET.fromstring(response.content))\n",
    "        ET.indent(xml, space=\"\\t\", level=0)\n",
    "\n",
    "        filename = id.replace('/', '')\n",
    "        with open(f'{DATA_DIR}{XML_DIR}{filename}.xml', 'w'): pass\n",
    "        xml.write(f'{DATA_DIR}{XML_DIR}{filename}.xml')\n",
    "\n",
    "        return SUCCESS\n",
    "    \n",
    "    with futures.ThreadPoolExecutor(max_workers) as exec:\n",
    "        outcomes = list(exec.map(download_single, ids))\n",
    "\n",
    "    if verbose:\n",
    "        print(\n",
    "            f'Processed {len(ids)} ids in {time.time() - start:.2f} seconds:',\n",
    "            f'{outcomes.count(SUCCESS)} downloaded,', \n",
    "            f'{outcomes.count(SKIP)} already present,', \n",
    "            f'{outcomes.count(FAIL)} failed'\n",
    "        )\n",
    "    return outcomes.count(SUCCESS) + outcomes.count(FAIL)\n",
    "\n",
    "id_list: list[str] = []\n",
    "with open(f'{DATA_DIR}{TXT_PATH}', 'r') as fp:\n",
    "    while(id := fp.readline()):\n",
    "        id_list.append(id[:-1])\n",
    "        \n",
    "n_success = download_many_xml(id_list[300:310])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
